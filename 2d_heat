{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":194069625,"sourceType":"kernelVersion"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch~=2.5.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n    ","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:01:50.675789Z","iopub.execute_input":"2024-11-04T02:01:50.676230Z","iopub.status.idle":"2024-11-04T02:02:02.477482Z","shell.execute_reply.started":"2024-11-04T02:01:50.676195Z","shell.execute_reply":"2024-11-04T02:02:02.476667Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: https://storage.googleapis.com/libtpu-releases/index.html\nCollecting torch~=2.5.0\n  Downloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m^C\n\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m178.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# os.environ['XLA_USE_BF16']=\"1\"\n# os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport random\nfrom torch.optim import LBFGS\nfrom tqdm import tqdm\n\n# for TPU\nimport torch_xla as xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.runtime as xmR\n\nfrom utility_py import *\n\nclass PINNs(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_layer):\n        super(PINNs, self).__init__()\n\n        layers = []\n        for i in range(num_layer-1):\n            if i == 0:\n                layers.append(nn.Linear(in_features=in_dim, out_features=hidden_dim))\n                layers.append(nn.Tanh())\n            else:\n                layers.append(nn.Linear(in_features=hidden_dim, out_features=hidden_dim))\n                layers.append(nn.Tanh())\n\n        layers.append(nn.Linear(in_features=hidden_dim, out_features=out_dim))\n\n        self.linear = nn.Sequential(*layers)\n\n    def forward(self, x, t):\n        src = torch.cat((x,t), dim=-1)\n        return self.linear(src)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.479152Z","iopub.execute_input":"2024-11-04T02:02:02.479407Z","iopub.status.idle":"2024-11-04T02:02:02.906115Z","shell.execute_reply.started":"2024-11-04T02:02:02.479379Z","shell.execute_reply":"2024-11-04T02:02:02.902557Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# implementation of PINNsformer\n# paper: PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks\n# link: https://arxiv.org/abs/2307.11833\n\nimport pdb\n\n#from utility_py import get_clones\n\nclass WaveAct(nn.Module):\n    def __init__(self):\n        super(WaveAct, self).__init__() \n        self.w1 = nn.Parameter(torch.ones(1), requires_grad=True)\n        self.w2 = nn.Parameter(torch.ones(1), requires_grad=True)\n\n    def forward(self, x):\n        return self.w1 * torch.sin(x)+ self.w2 * torch.cos(x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=256):\n        super(FeedForward, self).__init__() \n        self.linear = nn.Sequential(*[\n            nn.Linear(d_model, d_ff),\n            WaveAct(),\n            nn.Linear(d_ff, d_ff),\n            WaveAct(),\n            nn.Linear(d_ff, d_model)\n        ])\n\n    def forward(self, x):\n        return self.linear(x)\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads):\n        super(EncoderLayer, self).__init__()\n\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n        self.ff = FeedForward(d_model)\n        self.act1 = WaveAct()\n        self.act2 = WaveAct()\n        \n    def forward(self, x):\n        x2 = self.act1(x)\n        # pdb.set_trace()\n        x = x + self.attn(x2,x2,x2)[0]\n        x2 = self.act2(x)\n        x = x + self.ff(x2)\n        return x\n\n\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads):\n        super(DecoderLayer, self).__init__()\n\n        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=heads, batch_first=True)\n        self.ff = FeedForward(d_model)\n        self.act1 = WaveAct()\n        self.act2 = WaveAct()\n\n    def forward(self, x, e_outputs): \n        x2 = self.act1(x)\n        x = x + self.attn(x2, e_outputs, e_outputs)[0]\n        x2 = self.act2(x)\n        x = x + self.ff(x2)\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super(Encoder, self).__init__()\n        self.N = N\n        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n        self.act = WaveAct()\n\n    def forward(self, x):\n        for i in range(self.N):\n            x = self.layers[i](x)\n        return self.act(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, d_model, N, heads):\n        super(Decoder, self).__init__()\n        self.N = N\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.act = WaveAct()\n        \n    def forward(self, x, e_outputs):\n        for i in range(self.N):\n            x = self.layers[i](x, e_outputs)\n        return self.act(x)\n\n\n\nclass PINNsformer(nn.Module):\n    def __init__(self, d_out, d_model, d_hidden, N, heads):\n        super(PINNsformer, self).__init__()\n\n        self.linear_emb = nn.Linear(3, d_model)\n\n        self.encoder = Encoder(d_model, N, heads)\n        self.decoder = Decoder(d_model, N, heads)\n        self.linear_out = nn.Sequential(*[\n            nn.Linear(d_model, d_hidden),\n            WaveAct(),\n            nn.Linear(d_hidden, d_hidden),\n            WaveAct(),\n            nn.Linear(d_hidden, d_out)\n        ])\n\n    def forward(self, x, y, t):\n        src = torch.cat((x,y,t), dim=-1)\n        src = self.linear_emb(src)\n        e_outputs = self.encoder(src)\n        d_output = self.decoder(src, e_outputs)\n        output = self.linear_out(d_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.908444Z","iopub.status.idle":"2024-11-04T02:02:02.909181Z","shell.execute_reply.started":"2024-11-04T02:02:02.908797Z","shell.execute_reply":"2024-11-04T02:02:02.908834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 0.4  # Thermal diffusivity\nL = 1  # Length of the bar\nn = 1  # Frequency of the sinusoidal initial conditions\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = model.to(device)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.911091Z","iopub.status.idle":"2024-11-04T02:02:02.911784Z","shell.execute_reply.started":"2024-11-04T02:02:02.911412Z","shell.execute_reply":"2024-11-04T02:02:02.911448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 1\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.is_available() \n\ndef is_tpu_available():\n    return 'TPU_NAME' in os.environ\n\nif is_tpu_available():\n    print(\"TPU is available.\")\nelse:\n    #print(os.environ)\n    print(xmR.device_type())\n    \n    \ndevice =  xm.xla_device(devkind = 'TPU')\nprint(device)\n\ntorch.set_default_tensor_type('torch.FloatTensor')\nstep_size = 1e-3","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.913799Z","iopub.status.idle":"2024-11-04T02:02:02.914491Z","shell.execute_reply.started":"2024-11-04T02:02:02.914135Z","shell.execute_reply":"2024-11-04T02:02:02.914172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nres, b_left, b_right, b_upper, b_lower = get_data_3d([0, L],[0, L], [0,1],21, 21,21)\n\n\n# Extend time sequence\nres = make_time_sequence(res, num_step=2, step=step_size)\nb_left = make_time_sequence(b_left, num_step=2, step=step_size)\nb_right = make_time_sequence(b_right, num_step=2, step=step_size)\nb_upper = make_time_sequence(b_upper, num_step=2, step=step_size)\nb_lower = make_time_sequence(b_lower, num_step=2, step=step_size)\n\n# Convert to PyTorch tensors\nres = torch.tensor(res, dtype=torch.float32, requires_grad=True).to(xla.device())\nb_left = torch.tensor(b_left, dtype=torch.float32, requires_grad=True).to(xla.device())\nb_right = torch.tensor(b_right, dtype=torch.float32, requires_grad=True).to(xla.device())\nb_upper = torch.tensor(b_upper, dtype=torch.float32, requires_grad=True).to(xla.device())\nb_lower = torch.tensor(b_lower, dtype=torch.float32, requires_grad=True).to(xla.device())\n\n\n\n# Separate spatial (x) and temporal (t) components\n# Extract the (x, y, t) components\nx_res, y_res, t_res = res[:, :, 0:1], res[:, :, 1:2], res[:, :, 2:3]\nx_left, y_left, t_left = b_left[:, :, 0:1], b_left[:, :, 1:2], b_left[:, :, 2:3]\nx_right, y_right, t_right = b_right[:, :, 0:1], b_right[:, :, 1:2], b_right[:, :, 2:3]\nx_upper, y_upper, t_upper = b_upper[:, :, 0:1], b_upper[:, :, 1:2], b_upper[:, :, 2:3]\nx_lower, y_lower, t_lower = b_lower[:, :, 0:1], b_lower[:, :, 1:2], b_lower[:, :, 2:3]\n\n\n# Initialize model weights\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.916127Z","iopub.status.idle":"2024-11-04T02:02:02.916797Z","shell.execute_reply.started":"2024-11-04T02:02:02.916442Z","shell.execute_reply":"2024-11-04T02:02:02.916475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PINNsformer(d_out=1, d_hidden=512, d_model=32, N=1, heads=2).to(device)\nimport torch.optim as optimer\nmodel.apply(init_weights)\nmodel.to(xla.device())\noptim = LBFGS(model.parameters(), line_search_fn='strong_wolfe')\n#optim = optimer.Adam(model.parameters(), lr=1e-4)\n\nprint(model)\nprint(get_n_params(model))\nprint(next(model.parameters()).device)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.919190Z","iopub.status.idle":"2024-11-04T02:02:02.919879Z","shell.execute_reply.started":"2024-11-04T02:02:02.919519Z","shell.execute_reply":"2024-11-04T02:02:02.919555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gc\n\n# model.cpu()\n# del model\n# gc.collect()\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.921980Z","iopub.status.idle":"2024-11-04T02:02:02.922672Z","shell.execute_reply.started":"2024-11-04T02:02:02.922315Z","shell.execute_reply":"2024-11-04T02:02:02.922352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(x_res)\n\ndef check_nan(tensor, name):\n    if torch.isnan(tensor).any():\n        print(f\"NaNs detected in {name}\")\nloss_track = []\nprint(\"Shape of res:\", res.shape)\nprint(\"Shape of b_left:\", b_left.shape)\nprint(\"Shape of b_right:\", b_right.shape)\nprint(\"Shape of b_upper:\", b_upper.shape)\nprint(\"Shape of b_lower:\", b_lower.shape)\n# a = 0.4  # Thermal diffusivity\n# L = 1  # Length of the bar\n# n = 1  # Frequency of the sinusoidal initial conditions\nk= 0 \n# t_ic = torch.zeros_like(x_res)\n# bc_help = torch.ones_like(x_res)\nfor i in tqdm(range(750)):\n    k+=1\n\n    \n\n    def closure():\n        t_ic = torch.zeros_like(x_res, device=device)\n        bc_help = torch.ones_like(x_res, device=device)\n        \n        # Forward pass to calculate predictions\n        pred_res = model(x_res, y_res, t_res)\n        pred_ic = model(x_res, y_res, t_ic)\n        \n        # Calculate boundary condition predictions without tracking gradients\n        with torch.no_grad():\n            bc_y_max = model(x_res, bc_help, t_res)  # x,1,t\n            bc_x_max = model(bc_help, y_res, t_res)  # x,1,t\n            bc_y_min = model(x_res, t_ic, t_res)     # x,1,t\n            bc_x_min = model(t_ic, y_res, t_res)     # x,1,t\n\n        # Derivatives for the PDE residual\n        u_x = torch.autograd.grad(pred_res, x_res, grad_outputs=torch.ones_like(pred_res, device=device), retain_graph=True, create_graph=True)[0]\n        u_y = torch.autograd.grad(pred_res, y_res, grad_outputs=torch.ones_like(pred_res, device=device), retain_graph=True, create_graph=True)[0]\n        u_xx = torch.autograd.grad(u_x, x_res, grad_outputs=torch.ones_like(u_x, device=device), retain_graph=True, create_graph=True)[0]\n        u_yy = torch.autograd.grad(u_y, y_res, grad_outputs=torch.ones_like(u_y, device=device), retain_graph=True, create_graph=True)[0]\n        u_t = torch.autograd.grad(pred_res, t_res, grad_outputs=torch.ones_like(pred_res, device=device), retain_graph=True, create_graph=True)[0]\n\n        # Residual loss for the 2D heat equation\n        alpha = a\n        loss_res = torch.mean((u_t - alpha * (u_xx + u_yy)) ** 2)\n        \n        # Initial condition loss\n        loss_ic = torch.mean(pred_ic ** 2)\n        \n        # Boundary condition loss calculated without gradient tracking\n        with torch.no_grad():\n            loss_bc = torch.mean(bc_y_max ** 2 + bc_x_max ** 2 + bc_y_min ** 2 + bc_x_min ** 2)\n\n        # Track the losses\n        loss_track.append([\n            loss_res.detach().cpu().numpy(),\n            loss_ic.detach().cpu().numpy(),\n            loss_bc.detach().cpu().numpy()\n        ])\n\n        # Total loss\n        loss = 2 * loss_res + loss_ic + loss_bc\n        optim.zero_grad()\n        loss.backward()\n        return loss\n\n    xm.optimizer_step(optim, optimizer_args={'closure': closure})\n    xm.mark_step()\n    #xm.mark_step()\n    #print(loss_track)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.924976Z","iopub.status.idle":"2024-11-04T02:02:02.925661Z","shell.execute_reply.started":"2024-11-04T02:02:02.925311Z","shell.execute_reply":"2024-11-04T02:02:02.925347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Length of loss_track: {len(loss_track)}\")\n#print(loss_track)","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.928056Z","iopub.status.idle":"2024-11-04T02:02:02.928723Z","shell.execute_reply.started":"2024-11-04T02:02:02.928371Z","shell.execute_reply":"2024-11-04T02:02:02.928407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Loss Res: {:4f}, Loss_IC: {:4f}, loss_bc: {:4f}'.format(loss_track[-1][0], loss_track[-1][1], loss_track[-1][2]))\nprint('Train Loss: {:4f}'.format(np.sum(loss_track[-1])))\n\ntorch.save(model.state_dict(), './1dreaction_pinnsformer.pt')","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.930684Z","iopub.status.idle":"2024-11-04T02:02:02.931391Z","shell.execute_reply.started":"2024-11-04T02:02:02.931017Z","shell.execute_reply":"2024-11-04T02:02:02.931072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Convert loss_track to a NumPy array for easier slicing\nloss_track = np.array(loss_track)\n\n# Extract each loss component\nresidual_loss = loss_track[:, 0]\nboundary_condition_loss = loss_track[:, 2]\ninitial_condition_loss = loss_track[:, 1]\n\n# Plot loss components\nplt.figure(figsize=(12, 6))\nplt.plot(residual_loss, label='Residual Loss', color='r')\nplt.plot(boundary_condition_loss, label='Boundary Condition Loss', color='g')\nplt.plot(initial_condition_loss, label='Initial Condition Loss', color='b')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Loss Components During Optimization')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.933227Z","iopub.status.idle":"2024-11-04T02:02:02.933908Z","shell.execute_reply.started":"2024-11-04T02:02:02.933544Z","shell.execute_reply":"2024-11-04T02:02:02.933585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n\n\n# Generate the test data with the correct dimensions for the heat equation\nres_test, _, _, _, _ = get_data_3d([0, L], [0, L], [0, 1], 21, 21, 21)\n\n# Generate the test data with the correct dimensions for the heat equation\nres_test = make_time_sequence(res_test, num_step=2, step=1e-3) \nres_test = torch.tensor(res_test, dtype=torch.float32, requires_grad=True).to(device)\n\nx_test, y_test, t_test = res_test[:,:,0:1], res_test[:,:,1:2], res_test[:,:,2:3]\n\n# Predict using the model\nwith torch.no_grad():\n    pred = model(x_test, y_test, t_test)[:, 0:1]\n    pred = pred.cpu().detach().numpy()\n\n# Reshape the prediction to fit 19x19x19 grid (space and time)\npred = pred.reshape(31, 31, 31)\n\n# Define analytical solution for the 2D heat equation\n# u(x, y, t) = e^(-n^2 * pi^2 * alpha * t) * sin(n * pi * x / L) * sin(m * pi * y / L)\ndef u_ana(x, y, t, alpha=0.4, L=1, n=1, m=1):\n    return np.exp(-(n**2 * np.pi**2 * alpha * t) / (L**2)) * np.sin(n * np.pi * x / L) * np.sin(m * np.pi * y / L)\n\n# Get the test data again for analytical solution comparison\nres_test, _, _, _, _ = get_data_3d([0, L], [0, L], [0, 1], 101, 101, 101)\n\n# Compute the analytical solution for the 2D heat equation\nu = u_ana(res_test[:,0], res_test[:,1], res_test[:,2])\n\n# Reshape the analytical solution to fit the 19x19x19 grid\nu = u.reshape(101, 101, 101)\n\n# # Compute relative errors (L1 and L2)\n# rl1 = np.sum(np.abs(u - pred)) / np.sum(np.abs(u))\n# rl2 = np.sqrt(np.sum((u - pred) ** 2) / np.sum(u ** 2))\n\n# print('Relative L1 error: {:4f}'.format(rl1))\n# print('Relative L2 error: {:4f}'.format(rl2))\n\n# Visualization of the predicted solution u(x, y, t)\nplt.figure(figsize=(4, 3))\nplt.imshow(pred[:, :, 0], extent=[0, L, 0, 1], aspect='auto')  # Fix one time step for visualization\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Predicted u(x, y, t) - 2D Heat Equation')\nplt.colorbar()\nplt.tight_layout()\nplt.savefig('./2dheat_pinnsformer_pred.png')\nplt.show()\n\n# Visualization of the analytical solution u_ana(x, y, t)\nplt.figure(figsize=(4, 3))\nplt.imshow(u[:, :, 0], extent=[0, L, 0, 1], aspect='auto')  # Fix one time step for visualization\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Analytical u(x, y, t) - 2D Heat Equation')\nplt.colorbar()\nplt.tight_layout()\nplt.savefig('./2dheat_analytical.png')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-04T02:02:02.936629Z","iopub.status.idle":"2024-11-04T02:02:02.937334Z","shell.execute_reply.started":"2024-11-04T02:02:02.936952Z","shell.execute_reply":"2024-11-04T02:02:02.936987Z"},"trusted":true},"execution_count":null,"outputs":[]}]}